{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# miscellaneous\n",
    "import bisect\n",
    "import builtins\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# onnx\n",
    "import onnx\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from numpy import random as ra\n",
    "from torch.nn.parallel.parallel_apply import parallel_apply\n",
    "from torch.nn.parallel.replicate import replicate\n",
    "from torch.nn.parallel.scatter_gather import gather, scatter\n",
    "\n",
    "### import packages ###\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import collections\n",
    "import argparse\n",
    "\n",
    "\n",
    "import os.path\n",
    "from io import StringIO\n",
    "from os import path\n",
    "\n",
    "\n",
    "from torchviz import make_dot\n",
    "import torch.nn.functional as Functional\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "# others\n",
    "import collections\n",
    "\n",
    "from numpy import random as ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exc = getattr(builtins, \"IOError\", \"FileNotFoundError\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define dlrm in PyTorch ###\n",
    "class DLRM_Net(nn.Module):\n",
    "    def create_mlp(self, ln, sigmoid_layer):\n",
    "        # build MLP layer by layer\n",
    "        layers = nn.ModuleList()\n",
    "        for i in range(0, ln.size - 1):\n",
    "            n = ln[i]\n",
    "            m = ln[i + 1]\n",
    "\n",
    "            # construct fully connected operator\n",
    "            LL = nn.Linear(int(n), int(m), bias=True)\n",
    "\n",
    "            # initialize the weights\n",
    "            # with torch.no_grad():\n",
    "            # custom Xavier input, output or two-sided fill\n",
    "            mean = 0.0  # std_dev = np.sqrt(variance)\n",
    "            std_dev = np.sqrt(2 / (m + n))  # np.sqrt(1 / m) # np.sqrt(1 / n)\n",
    "            W = np.random.normal(mean, std_dev, size=(m, n)).astype(np.float32)\n",
    "            std_dev = np.sqrt(1 / m)  # np.sqrt(2 / (m + 1))\n",
    "            bt = np.random.normal(mean, std_dev, size=m).astype(np.float32)\n",
    "            # approach 1\n",
    "            LL.weight.data = torch.tensor(W, requires_grad=True)\n",
    "            LL.bias.data = torch.tensor(bt, requires_grad=True)\n",
    "            # approach 2\n",
    "            # LL.weight.data.copy_(torch.tensor(W))\n",
    "            # LL.bias.data.copy_(torch.tensor(bt))\n",
    "            # approach 3\n",
    "            # LL.weight = Parameter(torch.tensor(W),requires_grad=True)\n",
    "            # LL.bias = Parameter(torch.tensor(bt),requires_grad=True)\n",
    "            layers.append(LL)\n",
    "\n",
    "            # construct sigmoid or relu operator\n",
    "            if i == sigmoid_layer:\n",
    "                layers.append(nn.Sigmoid())\n",
    "            else:\n",
    "                layers.append(nn.ReLU())\n",
    "\n",
    "        # approach 1: use ModuleList\n",
    "        # return layers\n",
    "        # approach 2: use Sequential container to wrap all layers\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "    def create_emb(self, m, ln):\n",
    "        emb_l = nn.ModuleList()\n",
    "        for i in range(0, ln.size):\n",
    "            n = ln[i]\n",
    "\n",
    "            # construct embedding operator\n",
    "            EE = nn.EmbeddingBag(n, m, mode=\"sum\", sparse=True)\n",
    "            # initialize embeddings\n",
    "            # nn.init.uniform_(EE.weight, a=-np.sqrt(1 / n), b=np.sqrt(1 / n))\n",
    "            W = np.random.uniform(\n",
    "                low=-np.sqrt(1 / n), high=np.sqrt(1 / n), size=(n, m)\n",
    "            ).astype(np.float32)\n",
    "            # approach 1\n",
    "            EE.weight.data = torch.tensor(W, requires_grad=True)\n",
    "            # approach 2\n",
    "            # EE.weight.data.copy_(torch.tensor(W))\n",
    "            # approach 3\n",
    "            # EE.weight = Parameter(torch.tensor(W),requires_grad=True)\n",
    "            emb_l.append(EE)\n",
    "\n",
    "        return emb_l\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        m_spa=None,\n",
    "        ln_emb=None,\n",
    "        ln_bot=None,\n",
    "        ln_top=None,\n",
    "        arch_interaction_op=None,\n",
    "        arch_interaction_itself=False,\n",
    "        sigmoid_bot=-1,\n",
    "        sigmoid_top=-1,\n",
    "        sync_dense_params=True,\n",
    "        loss_threshold=0.0,\n",
    "        ndevices=-1,\n",
    "    ):\n",
    "        super(DLRM_Net, self).__init__()\n",
    "\n",
    "        if (\n",
    "            (m_spa is not None)\n",
    "            and (ln_emb is not None)\n",
    "            and (ln_bot is not None)\n",
    "            and (ln_top is not None)\n",
    "            and (arch_interaction_op is not None)\n",
    "        ):\n",
    "\n",
    "            # save arguments\n",
    "            self.ndevices = ndevices\n",
    "            self.output_d = 0\n",
    "            self.parallel_model_batch_size = -1\n",
    "            self.parallel_model_is_not_prepared = True\n",
    "            self.arch_interaction_op = arch_interaction_op\n",
    "            self.arch_interaction_itself = arch_interaction_itself\n",
    "            self.sync_dense_params = sync_dense_params\n",
    "            self.loss_threshold = loss_threshold\n",
    "            # create operators\n",
    "            self.emb_l = self.create_emb(m_spa, ln_emb)\n",
    "            self.bot_l = self.create_mlp(ln_bot, sigmoid_bot)\n",
    "            self.top_l = self.create_mlp(ln_top, sigmoid_top)\n",
    "\n",
    "    def apply_mlp(self, x, layers):\n",
    "        # approach 1: use ModuleList\n",
    "        # for layer in layers:\n",
    "        #     x = layer(x)\n",
    "        # return x\n",
    "        # approach 2: use Sequential container to wrap all layers\n",
    "        return layers(x)\n",
    "\n",
    "    def apply_emb(self, lS_o, lS_i, emb_l):\n",
    "        # WARNING: notice that we are processing the batch at once. We implicitly\n",
    "        # assume that the data is laid out such that:\n",
    "        # 1. each embedding is indexed with a group of sparse indices,\n",
    "        #   corresponding to a single lookup\n",
    "        # 2. for each embedding the lookups are further organized into a batch\n",
    "        # 3. for a list of embedding tables there is a list of batched lookups\n",
    "\n",
    "        ly = []\n",
    "        for k, sparse_index_group_batch in enumerate(lS_i):\n",
    "            sparse_offset_group_batch = lS_o[k]\n",
    "\n",
    "            # embedding lookup\n",
    "            # We are using EmbeddingBag, which implicitly uses sum operator.\n",
    "            # The embeddings are represented as tall matrices, with sum\n",
    "            # happening vertically across 0 axis, resulting in a row vector\n",
    "            E = emb_l[k]\n",
    "            V = E(sparse_index_group_batch, sparse_offset_group_batch)\n",
    "            \n",
    "            ly.append(V)\n",
    "\n",
    "        # print(ly)\n",
    "        return ly\n",
    "\n",
    "    def interact_features(self, x, ly):\n",
    "        if self.arch_interaction_op == \"dot\":\n",
    "            # concatenate dense and sparse features\n",
    "            if len(x.shape) != 1:\n",
    "                (batch_size, d) = x.shape\n",
    "            else:\n",
    "                (batch_size, d) = (x.shape, 0)\n",
    "            \n",
    "            T = torch.cat([x] + ly, dim=1).view((batch_size, -1, d))\n",
    "            # perform a dot product\n",
    "            Z = torch.bmm(T, torch.transpose(T, 1, 2))\n",
    "            # append dense feature with the interactions (into a row vector)\n",
    "            # approach 1: all\n",
    "            # Zflat = Z.view((batch_size, -1))\n",
    "            # approach 2: unique\n",
    "            _, ni, nj = Z.shape\n",
    "            offset = 0 if self.arch_interaction_itself else -1\n",
    "            li, lj = torch.tril_indices(ni, nj, offset=offset)\n",
    "            Zflat = Z[:, li, lj]\n",
    "            # concatenate dense features and interactions\n",
    "            R = torch.cat([x] + [Zflat], dim=1)\n",
    "        elif self.arch_interaction_op == \"cat\":\n",
    "            # concatenation features (into a row vector)\n",
    "            R = torch.cat([x] + ly, dim=1)\n",
    "        else:\n",
    "            sys.exit(\n",
    "                \"ERROR: --arch-interaction-op=\"\n",
    "                + self.arch_interaction_op\n",
    "                + \" is not supported\"\n",
    "            )\n",
    "\n",
    "        return R\n",
    "\n",
    "    def forward(self, dense_x, lS_o, lS_i):\n",
    "        if self.ndevices <= 1:\n",
    "            return self.sequential_forward(dense_x, lS_o, lS_i)\n",
    "        else:\n",
    "            return self.parallel_forward(dense_x, lS_o, lS_i)\n",
    "\n",
    "    def sequential_forward(self, dense_x, lS_o, lS_i):\n",
    "        # process dense features (using bottom mlp), resulting in a row vector\n",
    "        x = self.apply_mlp(dense_x, self.bot_l)\n",
    "#         print(\"x\", x)\n",
    "\n",
    "        # process sparse features(using embeddings), resulting in a list of row vectors\n",
    "        ly = self.apply_emb(lS_o, lS_i, self.emb_l)\n",
    "#         print(\"ly\", ly)\n",
    "\n",
    "        # interact features (dense and sparse)\n",
    "        z = self.interact_features(x, ly)\n",
    "\n",
    "        # obtain probability of a click (using top mlp)\n",
    "        p = self.apply_mlp(z, self.top_l)\n",
    "\n",
    "        # clamp output if needed\n",
    "        if 0.0 < self.loss_threshold and self.loss_threshold < 1.0:\n",
    "            z = torch.clamp(p, min=self.loss_threshold, max=(1.0 - self.loss_threshold))\n",
    "        else:\n",
    "            z = p\n",
    "\n",
    "        return z\n",
    "\n",
    "    def parallel_forward(self, dense_x, lS_o, lS_i):\n",
    "        ### prepare model (overwrite) ###\n",
    "        # WARNING: # of devices must be >= batch size in parallel_forward call\n",
    "        batch_size = dense_x.size()[0]\n",
    "        ndevices = min(self.ndevices, batch_size, len(self.emb_l))\n",
    "        device_ids = range(ndevices)\n",
    "        # WARNING: must redistribute the model if mini-batch size changes(this is common\n",
    "        # for last mini-batch, when # of elements in the dataset/batch size is not even\n",
    "        if self.parallel_model_batch_size != batch_size:\n",
    "            self.parallel_model_is_not_prepared = True\n",
    "\n",
    "        if self.sync_dense_params or self.parallel_model_is_not_prepared:\n",
    "            # replicate mlp (data parallelism)\n",
    "            self.bot_l_replicas = replicate(self.bot_l, device_ids)\n",
    "            self.top_l_replicas = replicate(self.top_l, device_ids)\n",
    "            # distribute embeddings (model parallelism)\n",
    "            t_list = []\n",
    "            for k, emb in enumerate(self.emb_l):\n",
    "                d = torch.device(\"cuda:\" + str(k % ndevices))\n",
    "                emb.to(d)\n",
    "                t_list.append(emb.to(d))\n",
    "            self.emb_l = nn.ModuleList(t_list)\n",
    "            self.parallel_model_batch_size = batch_size\n",
    "            self.parallel_model_is_not_prepared = False\n",
    "\n",
    "        ### prepare input (overwrite) ###\n",
    "        # scatter dense features (data parallelism)\n",
    "        # print(dense_x.device)\n",
    "        dense_x = scatter(dense_x, device_ids, dim=0)\n",
    "        # distribute sparse features (model parallelism)\n",
    "        if (len(self.emb_l) != len(lS_o)) or (len(self.emb_l) != len(lS_i)):\n",
    "            sys.exit(\"ERROR: corrupted model input detected in parallel_forward call\")\n",
    "\n",
    "        t_list = []\n",
    "        i_list = []\n",
    "        for k, _ in enumerate(self.emb_l):\n",
    "            d = torch.device(\"cuda:\" + str(k % ndevices))\n",
    "            t_list.append(lS_o[k].to(d))\n",
    "            i_list.append(lS_i[k].to(d))\n",
    "        lS_o = t_list\n",
    "        lS_i = i_list\n",
    "\n",
    "        ### compute results in parallel ###\n",
    "        # bottom mlp\n",
    "        # WARNING: Note that the self.bot_l is a list of bottom mlp modules\n",
    "        # that have been replicated across devices, while dense_x is a tuple of dense\n",
    "        # inputs that has been scattered across devices on the first (batch) dimension.\n",
    "        # The output is a list of tensors scattered across devices according to the\n",
    "        # distribution of dense_x.\n",
    "        x = parallel_apply(self.bot_l_replicas, dense_x, None, device_ids)\n",
    "        # debug prints\n",
    "        # print(x)\n",
    "\n",
    "        # embeddings\n",
    "        ly = self.apply_emb(lS_o, lS_i, self.emb_l)\n",
    "        # debug prints\n",
    "        # print(ly)\n",
    "\n",
    "        # butterfly shuffle (implemented inefficiently for now)\n",
    "        # WARNING: Note that at this point we have the result of the embedding lookup\n",
    "        # for the entire batch on each device. We would like to obtain partial results\n",
    "        # corresponding to all embedding lookups, but part of the batch on each device.\n",
    "        # Therefore, matching the distribution of output of bottom mlp, so that both\n",
    "        # could be used for subsequent interactions on each device.\n",
    "        if len(self.emb_l) != len(ly):\n",
    "            sys.exit(\"ERROR: corrupted intermediate result in parallel_forward call\")\n",
    "\n",
    "        t_list = []\n",
    "        for k, _ in enumerate(self.emb_l):\n",
    "            d = torch.device(\"cuda:\" + str(k % ndevices))\n",
    "            y = scatter(ly[k], device_ids, dim=0)\n",
    "            t_list.append(y)\n",
    "        # adjust the list to be ordered per device\n",
    "        ly = list(map(lambda y: list(y), zip(*t_list)))\n",
    "        # debug prints\n",
    "        # print(ly)\n",
    "\n",
    "        # interactions\n",
    "        z = []\n",
    "        for k in range(ndevices):\n",
    "            zk = self.interact_features(x[k], ly[k])\n",
    "            z.append(zk)\n",
    "        # debug prints\n",
    "        # print(z)\n",
    "\n",
    "        # top mlp\n",
    "        # WARNING: Note that the self.top_l is a list of top mlp modules that\n",
    "        # have been replicated across devices, while z is a list of interaction results\n",
    "        # that by construction are scattered across devices on the first (batch) dim.\n",
    "        # The output is a list of tensors scattered across devices according to the\n",
    "        # distribution of z.\n",
    "        p = parallel_apply(self.top_l_replicas, z, None, device_ids)\n",
    "\n",
    "        ### gather the distributed results ###\n",
    "        p0 = gather(p, self.output_d, dim=0)\n",
    "\n",
    "        # clamp output if needed\n",
    "        if 0.0 < self.loss_threshold and self.loss_threshold < 1.0:\n",
    "            z0 = torch.clamp(\n",
    "                p0, min=self.loss_threshold, max=(1.0 - self.loss_threshold)\n",
    "            )\n",
    "        else:\n",
    "            z0 = p0\n",
    "\n",
    "        return z0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data Utils\n",
    "\n",
    "def convertUStringToDistinctInts(mat, convertDicts, counts):\n",
    "    # Converts matrix of unicode strings into distinct integers.\n",
    "    #\n",
    "    # Inputs:\n",
    "    #     mat (np.array): array of unicode strings to convert\n",
    "    #     convertDicts (list): dictionary for each column\n",
    "    #     counts (list): number of different categories in each column\n",
    "    #\n",
    "    # Outputs:\n",
    "    #     out (np.array): array of output integers\n",
    "    #     convertDicts (list): dictionary for each column\n",
    "    #     counts (list): number of different categories in each column\n",
    "\n",
    "    # check if convertDicts and counts match correct length of mat\n",
    "    if len(convertDicts) != mat.shape[1] or len(counts) != mat.shape[1]:\n",
    "        print(\"Length of convertDicts or counts does not match input shape\")\n",
    "        print(\"Generating convertDicts and counts...\")\n",
    "\n",
    "        convertDicts = [{} for _ in range(mat.shape[1])]\n",
    "        counts = [0 for _ in range(mat.shape[1])]\n",
    "\n",
    "    # initialize output\n",
    "    out = torch.zeros(mat.shape)\n",
    "\n",
    "    for j in range(mat.shape[1]):\n",
    "        for i in range(mat.shape[0]):\n",
    "            # add to convertDict and increment count\n",
    "            if mat[i, j] not in convertDicts[j]:\n",
    "                convertDicts[j][mat[i, j]] = counts[j]\n",
    "                counts[j] += 1\n",
    "            out[i, j] = convertDicts[j][mat[i, j]]\n",
    "\n",
    "    return out, convertDicts, counts\n",
    "\n",
    "\n",
    "def processKaggleCriteoAdData(split, d_path):\n",
    "    # Process Kaggle Display Advertising Challenge Dataset by converting unicode strings\n",
    "    # in X_cat to integers and converting negative integer values in X_int.\n",
    "    #\n",
    "    # Loads data in the form \"kaggle_day_i.npz\" where i is the day.\n",
    "    #\n",
    "    # Inputs:\n",
    "    #   split (int): total number of splits in the dataset (typically 7)\n",
    "    #   d_path (str): path for kaggle_day_i.npz files\n",
    "\n",
    "    convertDicts = []\n",
    "    counts = []\n",
    "\n",
    "    # check if processed file already exists\n",
    "    idx = 1\n",
    "    while idx <= split:\n",
    "        if path.exists(str(d_path) + \"kaggle_day_{0}_processed.npz\".format(idx)):\n",
    "            idx += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # process data if not all files exist\n",
    "    if idx <= split:\n",
    "        for i in range(1, split + 1):\n",
    "            with np.load(str(d_path) + \"kaggle_day_{0}.npz\".format(i)) as data:\n",
    "\n",
    "                X_cat, convertDicts, counts = convertUStringToDistinctInts(\n",
    "                    data[\"X_cat\"], convertDicts, counts\n",
    "                )\n",
    "                X_int = data[\"X_int\"]\n",
    "                X_int[X_int < 0] = 0\n",
    "                y = data[\"y\"]\n",
    "\n",
    "            np.savez_compressed(\n",
    "                str(d_path) + \"kaggle_day_{0}_processed.npz\".format(i),\n",
    "                X_cat=X_cat,\n",
    "                X_int=X_int,\n",
    "                y=y,\n",
    "            )\n",
    "            print(\"Processed kaggle_day_{0}.npz...\".format(i), end=\"\\r\")\n",
    "\n",
    "        np.savez_compressed(str(d_path) + \"kaggle_counts.npz\", counts=counts)\n",
    "    else:\n",
    "        print(\"Using existing %skaggle_day_*_processed.npz files\" % str(d_path))\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def concatKaggleCriteoAdData(split, d_path, o_filename):\n",
    "    # Concatenates different days of Kaggle data and saves.\n",
    "    #\n",
    "    # Inputs:\n",
    "    #   split (int): total number of splits in the dataset (typically 7)\n",
    "    #   d_path (str): path for kaggle_day_i.npz files\n",
    "    #   o_filename (str): output file name\n",
    "    #\n",
    "    # Output:\n",
    "    #   o_file (str): output file path\n",
    "\n",
    "    print (\"Concatenating multiple day kaggle data into %s.npz file\" % str(d_path + o_filename))\n",
    "\n",
    "    # load and concatenate data\n",
    "    for i in range(1, split + 1):\n",
    "        with np.load(str(d_path) + \"kaggle_day_{0}_processed.npz\".format(i)) as data:\n",
    "\n",
    "            if i == 1:\n",
    "                X_cat = data[\"X_cat\"]\n",
    "                X_int = data[\"X_int\"]\n",
    "                y = data[\"y\"]\n",
    "\n",
    "            else:\n",
    "                X_cat = np.concatenate((X_cat, data[\"X_cat\"]))\n",
    "                X_int = np.concatenate((X_int, data[\"X_int\"]))\n",
    "                y = np.concatenate((y, data[\"y\"]))\n",
    "\n",
    "        print(\"Loaded day:\", i, \"y = 1:\", len(y[y == 1]), \"y = 0:\", len(y[y == 0]))\n",
    "\n",
    "    with np.load(str(d_path) + \"kaggle_counts.npz\") as data:\n",
    "\n",
    "        counts = data[\"counts\"]\n",
    "\n",
    "    print(\"Loaded counts!\")\n",
    "\n",
    "    np.savez_compressed(\n",
    "        str(d_path) + str(o_filename) + \".npz\",\n",
    "        X_cat=X_cat,\n",
    "        X_int=X_int,\n",
    "        y=y,\n",
    "        counts=counts,\n",
    "    )\n",
    "\n",
    "    return str(d_path) + str(o_filename) + \".npz\"\n",
    "\n",
    "\n",
    "def transformCriteoAdData(X_cat, X_int, y, split, randomize, cuda):\n",
    "    # Transforms Kaggle data by applying log transformation on dense features and\n",
    "    # converting everything to appropriate tensors.\n",
    "    #\n",
    "    # Inputs:\n",
    "    #     X_cat (ndarray): array of integers corresponding to preprocessed\n",
    "    #                      categorical features\n",
    "    #     X_int (ndarray): array of integers corresponding to dense features\n",
    "    #     y (ndarray): array of bool corresponding to labels\n",
    "    #     split (bool): flag for splitting dataset into training/validation/test\n",
    "    #                     sets\n",
    "    #     randomize (str): determines randomization scheme\n",
    "    #         \"none\": no randomization\n",
    "    #         \"day\": randomizes each day\"s data (only works if split = True)\n",
    "    #         \"total\": randomizes total dataset\n",
    "    #     cuda (bool): flag for enabling CUDA and transferring data to GPU\n",
    "    #\n",
    "    # Outputs:\n",
    "    #     if split:\n",
    "    #         X_cat_train (tensor): sparse features for training set\n",
    "    #         X_int_train (tensor): dense features for training set\n",
    "    #         y_train (tensor): labels for training set\n",
    "    #         X_cat_val (tensor): sparse features for validation set\n",
    "    #         X_int_val (tensor): dense features for validation set\n",
    "    #         y_val (tensor): labels for validation set\n",
    "    #         X_cat_test (tensor): sparse features for test set\n",
    "    #         X_int_test (tensor): dense features for test set\n",
    "    #         y_test (tensor): labels for test set\n",
    "    #     else:\n",
    "    #         X_cat (tensor): sparse features\n",
    "    #         X_int (tensor): dense features\n",
    "    #         y (tensor): label\n",
    "\n",
    "    # define initial set of indices\n",
    "    indices = np.arange(len(y))\n",
    "\n",
    "    # split dataset\n",
    "    if split:\n",
    "\n",
    "        indices = np.array_split(indices, 7)\n",
    "\n",
    "        # randomize each day\"s dataset\n",
    "        if randomize == \"day\" or randomize == \"total\":\n",
    "            for i in range(len(indices)):\n",
    "                indices[i] = np.random.permutation(indices[i])\n",
    "\n",
    "        train_indices = np.concatenate(indices[:-1])\n",
    "        test_indices = indices[-1]\n",
    "        val_indices, test_indices = np.array_split(test_indices, 2)\n",
    "\n",
    "        print(\"Defined training and testing indices...\")\n",
    "\n",
    "        # randomize all data in training set\n",
    "        if randomize == \"total\":\n",
    "            train_indices = np.random.permutation(train_indices)\n",
    "            print(\"Randomized indices...\")\n",
    "\n",
    "        # create training, validation, and test sets\n",
    "        X_cat_train = X_cat[train_indices]\n",
    "        X_int_train = X_int[train_indices]\n",
    "        y_train = y[train_indices]\n",
    "\n",
    "        X_cat_val = X_cat[val_indices]\n",
    "        X_int_val = X_int[val_indices]\n",
    "        y_val = y[val_indices]\n",
    "\n",
    "        X_cat_test = X_cat[test_indices]\n",
    "        X_int_test = X_int[test_indices]\n",
    "        y_test = y[test_indices]\n",
    "\n",
    "        print(\"Split data according to indices...\")\n",
    "\n",
    "        # convert to tensors\n",
    "        if cuda:\n",
    "            X_cat_train = torch.tensor(X_cat_train, dtype=torch.long).pin_memory()\n",
    "            X_int_train = torch.log(\n",
    "                torch.tensor(X_int_train, dtype=torch.float) + 1\n",
    "            ).pin_memory()\n",
    "            y_train = torch.tensor(y_train.astype(np.float32)).pin_memory()\n",
    "\n",
    "            X_cat_val = torch.tensor(X_cat_val, dtype=torch.long).pin_memory()\n",
    "            X_int_val = torch.log(\n",
    "                torch.tensor(X_int_val, dtype=torch.float) + 1\n",
    "            ).pin_memory()\n",
    "            y_val = torch.tensor(y_val.astype(np.float32)).pin_memory()\n",
    "\n",
    "            X_cat_test = torch.tensor(X_cat_test, dtype=torch.long).pin_memory()\n",
    "            X_int_test = torch.log(\n",
    "                torch.tensor(X_int_test, dtype=torch.float) + 1\n",
    "            ).pin_memory()\n",
    "            y_test = torch.tensor(y_test.astype(np.float32)).pin_memory()\n",
    "        else:\n",
    "            X_cat_train = torch.tensor(X_cat_train, dtype=torch.long)\n",
    "            X_int_train = torch.log(torch.tensor(X_int_train, dtype=torch.float) + 1)\n",
    "            y_train = torch.tensor(y_train.astype(np.float32))\n",
    "\n",
    "            X_cat_val = torch.tensor(X_cat_val, dtype=torch.long)\n",
    "            X_int_val = torch.log(torch.tensor(X_int_val, dtype=torch.float) + 1)\n",
    "            y_val = torch.tensor(y_val.astype(np.float32))\n",
    "\n",
    "            X_cat_test = torch.tensor(X_cat_test, dtype=torch.long)\n",
    "            X_int_test = torch.log(torch.tensor(X_int_test, dtype=torch.float) + 1)\n",
    "            y_test = torch.tensor(y_test.astype(np.float32))\n",
    "\n",
    "        print(\"Converted to tensors...done!\")\n",
    "\n",
    "        return (\n",
    "            X_cat_train,\n",
    "            X_int_train,\n",
    "            y_train,\n",
    "            X_cat_val,\n",
    "            X_int_val,\n",
    "            y_val,\n",
    "            X_cat_test,\n",
    "            X_int_test,\n",
    "            y_test,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "\n",
    "        # randomize data\n",
    "        if randomize == \"total\":\n",
    "            indices = np.random.permutation(indices)\n",
    "\n",
    "            print(\"Randomized indices...\")\n",
    "\n",
    "        X_cat = torch.tensor(X_cat[indices], dtype=torch.long)\n",
    "        X_int = torch.log(torch.tensor(X_int[indices], dtype=torch.float) + 1)\n",
    "        y = torch.tensor(y[indices].astype(np.float32))\n",
    "\n",
    "        print(\"Converted to tensors...done!\")\n",
    "\n",
    "        return X_cat, X_int, y\n",
    "\n",
    "\n",
    "def getKaggleCriteoAdData(datafile=\"\", o_filename=\"\"):\n",
    "    # Passes through entire dataset and defines dictionaries for categorical\n",
    "    # features and determines the number of total categories.\n",
    "    #\n",
    "    # Inputs:\n",
    "    #    datafile : path to downloaded raw data file\n",
    "    #    o_filename (str): saves results under o_filename if filename is not \"\"\n",
    "    #\n",
    "    # Output:\n",
    "    #   o_file (str): output file path\n",
    "\n",
    "    d_path = \"./kaggle_data/\"\n",
    "\n",
    "    # determine if intermediate data path exists\n",
    "    if path.isdir(str(d_path)):\n",
    "        print(\"Saving intermediate data files at %s\" % (d_path))\n",
    "    else:\n",
    "        os.mkdir(str(d_path))\n",
    "        print(\"Created %s for storing intermediate data files\" % (d_path))\n",
    "\n",
    "    # determine if data file exists (train.txt)\n",
    "    if path.exists(str(datafile)):\n",
    "        print(\"Reading data from path=%s\" % (str(datafile)))\n",
    "    else:\n",
    "        print(\n",
    "            \"Path of Kaggle Display Ad Challenge Dataset is invalid; please download from https://labs.criteo.com/2014/09/kaggle-contest-dataset-now-available-academic-use/\"\n",
    "        )\n",
    "        exit(0)\n",
    "\n",
    "    # count number of datapoints in training set\n",
    "    total_count = 0\n",
    "    with open(str(datafile)) as f:\n",
    "        for _ in f:\n",
    "            total_count += 1\n",
    "\n",
    "    print(\"Total number of datapoints:\", total_count)\n",
    "\n",
    "    # determine length of split over 7 days\n",
    "    split = 1\n",
    "    num_data_per_split, extras = divmod(total_count, 7)\n",
    "\n",
    "    # generate tuple for dtype and filling values\n",
    "    type = np.dtype(\n",
    "        [(\"label\", (\"i4\", 1)), (\"int_feature\", (\"i4\", 13)), (\"cat_feature\", (\"U8\", 26))]\n",
    "    )\n",
    "\n",
    "    # initialize data to store\n",
    "    if extras > 0:\n",
    "        num_data_in_split = num_data_per_split + 1\n",
    "        extras -= 1\n",
    "\n",
    "    y = np.zeros(num_data_in_split, dtype=\"i4\")\n",
    "    X_int = np.zeros((num_data_in_split, 13), dtype=\"i4\")\n",
    "    X_cat = np.zeros((num_data_in_split, 26), dtype=\"U8\")\n",
    "\n",
    "    # check if files exist\n",
    "    while split <= 7:\n",
    "        if path.exists(str(str(d_path) + \"kaggle_day_{0}.npz\".format(split))):\n",
    "            split += 1\n",
    "        else:\n",
    "            split = 1\n",
    "            break\n",
    "\n",
    "    count = 0\n",
    "    if split == 1:\n",
    "        # load training data\n",
    "        with open(str(datafile)) as f:\n",
    "\n",
    "            for i, line in enumerate(f):\n",
    "\n",
    "                # store day\"s worth of data and reinitialize data\n",
    "                if i == (count + num_data_in_split):\n",
    "                    np.savez_compressed(\n",
    "                        str(d_path) + \"kaggle_day_{0}.npz\".format(split),\n",
    "                        X_int=X_int,\n",
    "                        X_cat=X_cat,\n",
    "                        y=y,\n",
    "                    )\n",
    "\n",
    "                    print(\"\\nSaved kaggle_day_{0}.npz!\".format(split))\n",
    "\n",
    "                    split += 1\n",
    "                    count += num_data_in_split\n",
    "\n",
    "                    if extras > 0:\n",
    "                        num_data_in_split = num_data_per_split + 1\n",
    "                        extras -= 1\n",
    "\n",
    "                    y = np.zeros(num_data_in_split, dtype=\"i4\")\n",
    "                    X_int = np.zeros((num_data_in_split, 13), dtype=\"i4\")\n",
    "                    X_cat = np.zeros((num_data_in_split, 26), dtype=\"U8\")\n",
    "\n",
    "                data = np.genfromtxt(StringIO(line), dtype=type, delimiter=\"\\t\")\n",
    "\n",
    "                y[i - count] = data[\"label\"]\n",
    "                X_int[i - count] = data[\"int_feature\"]\n",
    "                X_cat[i - count] = data[\"cat_feature\"]\n",
    "\n",
    "                print(\n",
    "                    \"Loading %d/%d   Split: %d   No Data in Split: %d  true label: %d  stored label: %d\"\n",
    "                    % (\n",
    "                        i,\n",
    "                        total_count,\n",
    "                        split,\n",
    "                        num_data_in_split,\n",
    "                        data[\"label\"],\n",
    "                        y[i - count],\n",
    "                    ),\n",
    "                    end=\"\\r\",\n",
    "                )\n",
    "\n",
    "        np.savez_compressed(\n",
    "            str(d_path) + \"kaggle_day_{0}.npz\".format(split),\n",
    "            X_int=X_int,\n",
    "            X_cat=X_cat,\n",
    "            y=y,\n",
    "        )\n",
    "\n",
    "        print(\"\\nSaved kaggle_day_{0}.npz!\".format(split))\n",
    "    else:\n",
    "        print(\"Using existing %skaggle_day_*.npz files\" % str(d_path))\n",
    "\n",
    "    processKaggleCriteoAdData(split, d_path)\n",
    "    o_file = concatKaggleCriteoAdData(split, d_path, o_filename)\n",
    "\n",
    "    return o_file\n",
    "\n",
    "\n",
    "def loadDataset(dataset, num_samples, df_path=\"\", data=\"\"):\n",
    "    if dataset == \"kaggle\":\n",
    "        df_exists = path.exists(str(data))\n",
    "        if df_exists:\n",
    "            print(\"Reading from pre-processed data=%s\" % (str(data)))\n",
    "            file = str(data)\n",
    "        else:\n",
    "            o_filename = \"kaggleAdDisplayChallenge_processed\"\n",
    "            file = getKaggleCriteoAdData(df_path, o_filename)\n",
    "    elif dataset == \"terabyte\":\n",
    "        file = \"./terbyte_data/tb_processed.npz\"\n",
    "        df_exists = path.exists(str(file))\n",
    "        if df_exists:\n",
    "            print(\"Reading Terabyte data-set processed data from %s\" % file)\n",
    "        else:\n",
    "            raise (\n",
    "                ValueError(\n",
    "                    \"Terabyte data-set processed data file %s does not exist !!\" % file\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # load and preprocess data\n",
    "    with np.load(file) as data:\n",
    "\n",
    "        X_int = data[\"X_int\"]\n",
    "        X_cat = data[\"X_cat\"]\n",
    "        y = data[\"y\"]\n",
    "        counts = data[\"counts\"]\n",
    "\n",
    "    return X_cat, X_int, y, counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Data loading functions\n",
    "\n",
    "# Kaggle Display Advertising Challenge Dataset\n",
    "# dataset (str): name of dataset (Kaggle or Terabyte)\n",
    "# randomize (str): determines randomization scheme\n",
    "#            \"none\": no randomization\n",
    "#            \"day\": randomizes each day\"s data (only works if split = True)\n",
    "#            \"total\": randomizes total dataset\n",
    "# split (bool) : to split into train, test, validation data-sets\n",
    "def read_dataset(\n",
    "    dataset,\n",
    "    mini_batch_size,\n",
    "    randomize,\n",
    "    num_batches,\n",
    "    split=True,\n",
    "    raw_data=\"\",\n",
    "    processed_data=\"\",\n",
    "    inference_only=False,\n",
    "):\n",
    "    # load\n",
    "    print(\"Loading %s dataset...\" % dataset)\n",
    "    nbatches = 0\n",
    "    num_samples = num_batches * mini_batch_size\n",
    "    X_cat, X_int, y, counts = data_utils.loadDataset(\n",
    "        dataset, num_samples, raw_data, processed_data\n",
    "    )\n",
    "\n",
    "    # transform\n",
    "    (\n",
    "        X_cat_train,\n",
    "        X_int_train,\n",
    "        y_train,\n",
    "        X_cat_val,\n",
    "        X_int_val,\n",
    "        y_val,\n",
    "        X_cat_test,\n",
    "        X_int_test,\n",
    "        y_test,\n",
    "    ) = data_utils.transformCriteoAdData(X_cat, X_int, y, split, randomize, False)\n",
    "    ln_emb = counts\n",
    "    m_den = X_int_train.shape[1]\n",
    "    n_emb = len(counts)\n",
    "    print(\"Sparse features = %d, Dense features = %d\" % (n_emb, m_den))\n",
    "\n",
    "    # adjust parameters\n",
    "    if not inference_only:\n",
    "        lX = []\n",
    "        lS_offsets = []\n",
    "        lS_indices = []\n",
    "        lT = []\n",
    "        train_nsamples = len(y_train)\n",
    "        data_size = train_nsamples\n",
    "        nbatches = int(np.floor((data_size * 1.0) / mini_batch_size))\n",
    "        print(\"Training data\")\n",
    "        if num_batches != 0 and num_batches < nbatches:\n",
    "            print(\n",
    "                \"Limiting to %d batches of the total % d batches\"\n",
    "                % (num_batches, nbatches)\n",
    "            )\n",
    "            nbatches = num_batches\n",
    "        else:\n",
    "            print(\"Total number of batches %d\" % nbatches)\n",
    "\n",
    "        # training data main loop\n",
    "        for j in range(0, nbatches):\n",
    "            # number of data points in a batch\n",
    "            print(\"Reading in batch: %d / %d\" % (j + 1, nbatches), end=\"\\r\")\n",
    "            n = min(mini_batch_size, data_size - (j * mini_batch_size))\n",
    "            # dense feature\n",
    "            idx_start = j * mini_batch_size\n",
    "            # WARNING: X_int_train is a PyTorch tensor\n",
    "            lX.append(\n",
    "                torch.tensor(\n",
    "                    (X_int_train[idx_start : (idx_start + n)])\n",
    "                    .numpy()\n",
    "                    .astype(np.float32)\n",
    "                )\n",
    "            )\n",
    "            # Training targets - ouptuts\n",
    "            # WARNING: y_train is a PyTorch tensor\n",
    "            lT.append(\n",
    "                torch.tensor(\n",
    "                    (y_train[idx_start : idx_start + n])\n",
    "                    .numpy()\n",
    "                    .reshape(-1, 1)\n",
    "                    .astype(np.float32)\n",
    "                )\n",
    "            )\n",
    "            # sparse feature (sparse indices)\n",
    "            lS_emb_indices = []\n",
    "            # for each embedding generate a list of n lookups,\n",
    "            # where each lookup is composed of multiple sparse indices\n",
    "            for size in range(n_emb):\n",
    "                lS_batch_indices = []\n",
    "                for _b in range(n):\n",
    "                    # WARNING: X_cat_train is a PyTorch tensor\n",
    "                    # store lengths and indices\n",
    "                    lS_batch_indices += (\n",
    "                        (X_cat_train[idx_start + _b][size].view(-1))\n",
    "                        .numpy()\n",
    "                        .astype(np.int64)\n",
    "                    ).tolist()\n",
    "                lS_emb_indices.append(torch.tensor(lS_batch_indices))\n",
    "            lS_indices.append(lS_emb_indices)\n",
    "            # Criteo Kaggle data it is 1 because data is categorical\n",
    "            lS_offsets.append([torch.tensor(list(range(n))) for _ in range(n_emb)])\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # adjust parameters\n",
    "    lX_test = []\n",
    "    lS_offsets_test = []\n",
    "    lS_indices_test = []\n",
    "    lT_test = []\n",
    "    test_nsamples = len(y_test)\n",
    "    data_size = test_nsamples\n",
    "    nbatches_test = int(np.floor((data_size * 1.0) / mini_batch_size))\n",
    "    print(\"Testing data\")\n",
    "    if num_batches != 0 and num_batches < nbatches_test:\n",
    "        print(\n",
    "            \"Limiting to %d batches of the total % d batches\"\n",
    "            % (num_batches, nbatches_test)\n",
    "        )\n",
    "        nbatches_test = num_batches\n",
    "    else:\n",
    "        print(\"Total number of batches %d\" % nbatches_test)\n",
    "\n",
    "    # testing data main loop\n",
    "    for j in range(0, nbatches_test):\n",
    "        # number of data points in a batch\n",
    "        print(\"Reading in batch: %d / %d\" % (j + 1, nbatches_test), end=\"\\r\")\n",
    "        n = min(mini_batch_size, data_size - (j * mini_batch_size))\n",
    "        # dense feature\n",
    "        idx_start = j * mini_batch_size\n",
    "        # WARNING: X_int_test is a PyTorch tensor\n",
    "        lX_test.append(\n",
    "            torch.tensor(\n",
    "                (X_int_test[idx_start : (idx_start + n)]).numpy().astype(np.float32)\n",
    "            )\n",
    "        )\n",
    "        # Training targets - ouptuts\n",
    "        # WARNING: y_test is a PyTorch tensor\n",
    "        lT_test.append(\n",
    "            torch.tensor(\n",
    "                (y_test[idx_start : idx_start + n])\n",
    "                .numpy()\n",
    "                .reshape(-1, 1)\n",
    "                .astype(np.float32)\n",
    "            )\n",
    "        )\n",
    "        # sparse feature (sparse indices)\n",
    "        lS_emb_indices = []\n",
    "        # for each embedding generate a list of n lookups,\n",
    "        # where each lookup is composed of multiple sparse indices\n",
    "        for size in range(n_emb):\n",
    "            lS_batch_indices = []\n",
    "            for _b in range(n):\n",
    "                # WARNING: X_cat_test is a PyTorch tensor\n",
    "                # store lengths and indices\n",
    "                lS_batch_indices += (\n",
    "                    (X_cat_test[idx_start + _b][size].view(-1)).numpy().astype(np.int64)\n",
    "                ).tolist()\n",
    "            lS_emb_indices.append(torch.tensor(lS_batch_indices))\n",
    "        lS_indices_test.append(lS_emb_indices)\n",
    "        # Criteo Kaggle data it is 1 because data is categorical\n",
    "        lS_offsets_test.append([torch.tensor(list(range(n))) for _ in range(n_emb)])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    if not inference_only:\n",
    "        return (\n",
    "            nbatches,\n",
    "            lX,\n",
    "            lS_offsets,\n",
    "            lS_indices,\n",
    "            lT,\n",
    "            nbatches_test,\n",
    "            lX_test,\n",
    "            lS_offsets_test,\n",
    "            lS_indices_test,\n",
    "            lT_test,\n",
    "            ln_emb,\n",
    "            m_den,\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            nbatches_test,\n",
    "            lX_test,\n",
    "            lS_offsets_test,\n",
    "            lS_indices_test,\n",
    "            lT_test,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            ln_emb,\n",
    "            m_den,\n",
    "        )\n",
    "\n",
    "\n",
    "# uniform ditribution (input data)\n",
    "def generate_random_input_data(\n",
    "    data_size,\n",
    "    num_batches,\n",
    "    mini_batch_size,\n",
    "    round_targets,\n",
    "    num_indices_per_lookup,\n",
    "    num_indices_per_lookup_fixed,\n",
    "    m_den,\n",
    "    ln_emb,\n",
    "):\n",
    "    nbatches = int(np.ceil((data_size * 1.0) / mini_batch_size))\n",
    "    if num_batches != 0:\n",
    "        nbatches = num_batches\n",
    "        data_size = nbatches * mini_batch_size\n",
    "    # print(\"Total number of batches %d\" % nbatches)\n",
    "\n",
    "    # inputs\n",
    "    lX = []\n",
    "    lS_offsets = []\n",
    "    lS_indices = []\n",
    "    for j in range(0, nbatches):\n",
    "        # number of data points in a batch\n",
    "        n = min(mini_batch_size, data_size - (j * mini_batch_size))\n",
    "        # dense feature\n",
    "        Xt = ra.rand(n, m_den).astype(np.float32)\n",
    "        lX.append(torch.tensor(Xt))\n",
    "        # sparse feature (sparse indices)\n",
    "        lS_emb_offsets = []\n",
    "        lS_emb_indices = []\n",
    "        # for each embedding generate a list of n lookups,\n",
    "        # where each lookup is composed of multiple sparse indices\n",
    "        for size in ln_emb:\n",
    "            lS_batch_offsets = []\n",
    "            lS_batch_indices = []\n",
    "            offset = 0\n",
    "            for _ in range(n):\n",
    "                # num of sparse indices to be used per embedding (between\n",
    "                if num_indices_per_lookup_fixed:\n",
    "                    sparse_group_size = np.int64(num_indices_per_lookup)\n",
    "                else:\n",
    "                    # random between [1,num_indices_per_lookup])\n",
    "                    r = ra.random(1)\n",
    "                    sparse_group_size = np.int64(\n",
    "                        np.round(max([1.0], r * min(size, num_indices_per_lookup)))\n",
    "                    )\n",
    "                # sparse indices to be used per embedding\n",
    "                r = ra.random(sparse_group_size)\n",
    "                sparse_group = np.unique(np.round(r * (size - 1)).astype(np.int64))\n",
    "                # reset sparse_group_size in case some index duplicates were removed\n",
    "                sparse_group_size = np.int64(sparse_group.size)\n",
    "                # store lengths and indices\n",
    "                lS_batch_offsets += [offset]\n",
    "                lS_batch_indices += sparse_group.tolist()\n",
    "                # update offset for next iteration\n",
    "                offset += sparse_group_size\n",
    "            lS_emb_offsets.append(torch.tensor(lS_batch_offsets))\n",
    "            lS_emb_indices.append(torch.tensor(lS_batch_indices))\n",
    "        lS_offsets.append(lS_emb_offsets)\n",
    "        lS_indices.append(lS_emb_indices)\n",
    "\n",
    "    return (nbatches, lX, lS_offsets, lS_indices)\n",
    "\n",
    "\n",
    "# uniform distribution (output data)\n",
    "def generate_random_output_data(\n",
    "    data_size, num_batches, mini_batch_size, num_targets=1, round_targets=False\n",
    "):\n",
    "    nbatches = int(np.ceil((data_size * 1.0) / mini_batch_size))\n",
    "    if num_batches != 0:\n",
    "        nbatches = num_batches\n",
    "        data_size = nbatches * mini_batch_size\n",
    "    # print(\"Total number of batches %d\" % nbatches)\n",
    "\n",
    "    lT = []\n",
    "    for j in range(0, nbatches):\n",
    "        # number of data points in a batch\n",
    "        n = min(mini_batch_size, data_size - (j * mini_batch_size))\n",
    "        # target (probability of a click)\n",
    "        if round_targets:\n",
    "            P = np.round(ra.rand(n, num_targets).astype(np.float32)).astype(np.float32)\n",
    "        else:\n",
    "            P = ra.rand(n, num_targets).astype(np.float32)\n",
    "        lT.append(torch.tensor(P))\n",
    "\n",
    "    return (nbatches, lT)\n",
    "\n",
    "\n",
    "# synthetic distribution (input data)\n",
    "def generate_synthetic_input_data(\n",
    "    data_size,\n",
    "    num_batches,\n",
    "    mini_batch_size,\n",
    "    round_targets,\n",
    "    num_indices_per_lookup,\n",
    "    num_indices_per_lookup_fixed,\n",
    "    m_den,\n",
    "    ln_emb,\n",
    "    trace_file,\n",
    "    enable_padding=False,\n",
    "):\n",
    "    nbatches = int(np.ceil((data_size * 1.0) / mini_batch_size))\n",
    "    if num_batches != 0:\n",
    "        nbatches = num_batches\n",
    "        data_size = nbatches * mini_batch_size\n",
    "    # print(\"Total number of batches %d\" % nbatches)\n",
    "\n",
    "    # inputs and targets\n",
    "    lX = []\n",
    "    lS_offsets = []\n",
    "    lS_indices = []\n",
    "    for j in range(0, nbatches):\n",
    "        # number of data points in a batch\n",
    "        n = min(mini_batch_size, data_size - (j * mini_batch_size))\n",
    "        # dense feature\n",
    "        Xt = ra.rand(n, m_den).astype(np.float32)\n",
    "        lX.append(torch.tensor(Xt))\n",
    "        # sparse feature (sparse indices)\n",
    "        lS_emb_offsets = []\n",
    "        lS_emb_indices = []\n",
    "        # for each embedding generate a list of n lookups,\n",
    "        # where each lookup is composed of multiple sparse indices\n",
    "        for i, size in enumerate(ln_emb):\n",
    "            lS_batch_offsets = []\n",
    "            lS_batch_indices = []\n",
    "            offset = 0\n",
    "            for _ in range(n):\n",
    "                # num of sparse indices to be used per embedding (between\n",
    "                if num_indices_per_lookup_fixed:\n",
    "                    sparse_group_size = np.int64(num_indices_per_lookup)\n",
    "                else:\n",
    "                    # random between [1,num_indices_per_lookup])\n",
    "                    r = ra.random(1)\n",
    "                    sparse_group_size = np.int64(\n",
    "                        max(1, np.round(r * min(size, num_indices_per_lookup))[0])\n",
    "                    )\n",
    "                # sparse indices to be used per embedding\n",
    "                file_path = trace_file\n",
    "                line_accesses, list_sd, cumm_sd = read_dist_from_file(\n",
    "                    file_path.replace(\"j\", str(i))\n",
    "                )\n",
    "                # debug prints\n",
    "                # print(\"input\")\n",
    "                # print(line_accesses); print(list_sd); print(cumm_sd);\n",
    "                # print(sparse_group_size)\n",
    "                # approach 1: rand\n",
    "                # r = trace_generate_rand(\n",
    "                #     line_accesses, list_sd, cumm_sd, sparse_group_size, enable_padding\n",
    "                # )\n",
    "                # approach 2: lru\n",
    "                r = trace_generate_lru(\n",
    "                    line_accesses, list_sd, cumm_sd, sparse_group_size, enable_padding\n",
    "                )\n",
    "                # WARNING: if the distribution in the file is not consistent\n",
    "                # with embedding table dimensions, below mod guards against out\n",
    "                # of range access\n",
    "                sparse_group = np.unique(r).astype(np.int64)\n",
    "                minsg = np.min(sparse_group)\n",
    "                maxsg = np.max(sparse_group)\n",
    "                if (minsg < 0) or (size <= maxsg):\n",
    "                    print(\n",
    "                        \"WARNING: distribution is inconsistent with embedding \"\n",
    "                        + \"table size (using mod to recover and continue)\"\n",
    "                    )\n",
    "                    sparse_group = np.mod(sparse_group, size).astype(np.int64)\n",
    "                # sparse_group = np.unique(np.array(np.mod(r, size-1)).astype(np.int64))\n",
    "                # reset sparse_group_size in case some index duplicates were removed\n",
    "                sparse_group_size = np.int64(sparse_group.size)\n",
    "                # store lengths and indices\n",
    "                lS_batch_offsets += [offset]\n",
    "                lS_batch_indices += sparse_group.tolist()\n",
    "                # update offset for next iteration\n",
    "                offset += sparse_group_size\n",
    "            lS_emb_offsets.append(torch.tensor(lS_batch_offsets))\n",
    "            lS_emb_indices.append(torch.tensor(lS_batch_indices))\n",
    "        lS_offsets.append(lS_emb_offsets)\n",
    "        lS_indices.append(lS_emb_indices)\n",
    "\n",
    "    return (nbatches, lX, lS_offsets, lS_indices)\n",
    "\n",
    "\n",
    "def generate_stack_distance(cumm_val, cumm_dist, max_i, i, enable_padding=False):\n",
    "    u = ra.rand(1)\n",
    "    if i < max_i:\n",
    "        # only generate stack distances up to the number of new references seen so far\n",
    "        j = bisect.bisect(cumm_val, i) - 1\n",
    "        fi = cumm_dist[j]\n",
    "        u *= fi  # shrink distribution support to exclude last values\n",
    "    elif enable_padding:\n",
    "        # WARNING: disable generation of new references (once all have been seen)\n",
    "        fi = cumm_dist[0]\n",
    "        u = (1.0 - fi) * u + fi  # remap distribution support to exclude first value\n",
    "\n",
    "    for (j, f) in enumerate(cumm_dist):\n",
    "        if u <= f:\n",
    "            return cumm_val[j]\n",
    "\n",
    "\n",
    "# WARNING: global define, must be consistent across all synthetic functions\n",
    "cache_line_size = 1\n",
    "\n",
    "\n",
    "def trace_generate_lru(\n",
    "    line_accesses, list_sd, cumm_sd, out_trace_len, enable_padding=False\n",
    "):\n",
    "    max_sd = list_sd[-1]\n",
    "    l = len(line_accesses)\n",
    "    i = 0\n",
    "    ztrace = []\n",
    "    for _ in range(out_trace_len):\n",
    "        sd = generate_stack_distance(list_sd, cumm_sd, max_sd, i, enable_padding)\n",
    "        mem_ref_within_line = 0  # floor(ra.rand(1)*cache_line_size) #0\n",
    "\n",
    "        # generate memory reference\n",
    "        if sd == 0:  # new reference #\n",
    "            line_ref = line_accesses.pop(0)\n",
    "            line_accesses.append(line_ref)\n",
    "            mem_ref = np.uint64(line_ref * cache_line_size + mem_ref_within_line)\n",
    "            i += 1\n",
    "        else:  # existing reference #\n",
    "            line_ref = line_accesses[l - sd]\n",
    "            mem_ref = np.uint64(line_ref * cache_line_size + mem_ref_within_line)\n",
    "            line_accesses.pop(l - sd)\n",
    "            line_accesses.append(line_ref)\n",
    "        # save generated memory reference\n",
    "        ztrace.append(mem_ref)\n",
    "\n",
    "    return ztrace\n",
    "\n",
    "\n",
    "def trace_generate_rand(\n",
    "    line_accesses, list_sd, cumm_sd, out_trace_len, enable_padding=False\n",
    "):\n",
    "    max_sd = list_sd[-1]\n",
    "    l = len(line_accesses)  # !!!Unique,\n",
    "    i = 0\n",
    "    ztrace = []\n",
    "    for _ in range(out_trace_len):\n",
    "        sd = generate_stack_distance(list_sd, cumm_sd, max_sd, i, enable_padding)\n",
    "        mem_ref_within_line = 0  # floor(ra.rand(1)*cache_line_size) #0\n",
    "        # generate memory reference\n",
    "        if sd == 0:  # new reference #\n",
    "            line_ref = line_accesses.pop(0)\n",
    "            line_accesses.append(line_ref)\n",
    "            mem_ref = np.uint64(line_ref * cache_line_size + mem_ref_within_line)\n",
    "            i += 1\n",
    "        else:  # existing reference #\n",
    "            line_ref = line_accesses[l - sd]\n",
    "            mem_ref = np.uint64(line_ref * cache_line_size + mem_ref_within_line)\n",
    "        ztrace.append(mem_ref)\n",
    "\n",
    "    return ztrace\n",
    "\n",
    "\n",
    "def trace_profile(trace, enable_padding=False):\n",
    "    # number of elements in the array (assuming 1D)\n",
    "    # n = trace.size\n",
    "\n",
    "    rstack = []  # S\n",
    "    stack_distances = []  # SDS\n",
    "    line_accesses = []  # L\n",
    "    for x in trace:\n",
    "        r = np.uint64(x / cache_line_size)\n",
    "        l = len(rstack)\n",
    "        try:  # found #\n",
    "            i = rstack.index(r)\n",
    "            # WARNING: I believe below is the correct depth in terms of meaning of the\n",
    "            #          algorithm, but that is not what seems to be in the paper alg.\n",
    "            #          -1 can be subtracted if we defined the distance between\n",
    "            #          consecutive accesses (e.g. r, r) as 0 rather than 1.\n",
    "            sd = l - i  # - 1\n",
    "            # push r to the end of stack_distances\n",
    "            stack_distances.insert(0, sd)\n",
    "            # remove r from its position and insert to the top of stack\n",
    "            rstack.pop(i)  # rstack.remove(r)\n",
    "            rstack.insert(l - 1, r)\n",
    "        except ValueError:  # not found #\n",
    "            sd = 0  # -1\n",
    "            # push r to the end of stack_distances/line_accesses\n",
    "            stack_distances.insert(0, sd)\n",
    "            line_accesses.insert(0, r)\n",
    "            # push r to the top of stack\n",
    "            rstack.insert(l, r)\n",
    "\n",
    "    if enable_padding:\n",
    "        # WARNING: notice that as the ratio between the number of samples (l)\n",
    "        # and cardinality (c) of a sample increases the probability of\n",
    "        # generating a sample gets smaller and smaller because there are\n",
    "        # few new samples compared to repeated samples. This means that for a\n",
    "        # long trace with relatively small cardinality it will take longer to\n",
    "        # generate all new samples and therefore obtain full distribution support\n",
    "        # and hence it takes longer for distribution to resemble the original.\n",
    "        # Therefore, we may pad the number of new samples to be on par with\n",
    "        # average number of samples l/c artificially.\n",
    "        l = len(stack_distances)\n",
    "        c = max(stack_distances)\n",
    "        padding = int(np.ceil(l / c))\n",
    "        stack_distances = stack_distances + [0] * padding\n",
    "\n",
    "    return (rstack, stack_distances, line_accesses)\n",
    "\n",
    "\n",
    "# auxiliary read/write routines\n",
    "def read_trace_from_file(file_path):\n",
    "    try:\n",
    "        with open(file_path) as f:\n",
    "            if args.trace_file_binary_type:\n",
    "                array = np.fromfile(f, dtype=np.uint64)\n",
    "                trace = array.astype(np.uint64).tolist()\n",
    "            else:\n",
    "                line = f.readline()\n",
    "                trace = list(map(lambda x: np.uint64(x), line.split(\", \")))\n",
    "            return trace\n",
    "    except Exception:\n",
    "        print(\"ERROR: no input trace file has been provided\")\n",
    "\n",
    "\n",
    "def write_trace_to_file(file_path, trace):\n",
    "    try:\n",
    "        if args.trace_file_binary_type:\n",
    "            with open(file_path, \"wb+\") as f:\n",
    "                np.array(trace).astype(np.uint64).tofile(f)\n",
    "        else:\n",
    "            with open(file_path, \"w+\") as f:\n",
    "                s = str(trace)\n",
    "                f.write(s[1 : len(s) - 1])\n",
    "    except Exception:\n",
    "        print(\"ERROR: no output trace file has been provided\")\n",
    "\n",
    "\n",
    "def read_dist_from_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            lines = f.read().splitlines()\n",
    "    except Exception:\n",
    "        print(\"Wrong file or file path\")\n",
    "    # read unique accesses\n",
    "    unique_accesses = [int(el) for el in lines[0].split(\", \")]\n",
    "    # read cumulative distribution (elements are passed as two separate lists)\n",
    "    list_sd = [int(el) for el in lines[1].split(\", \")]\n",
    "    cumm_sd = [float(el) for el in lines[2].split(\", \")]\n",
    "\n",
    "    return unique_accesses, list_sd, cumm_sd\n",
    "\n",
    "\n",
    "def write_dist_to_file(file_path, unique_accesses, list_sd, cumm_sd):\n",
    "    try:\n",
    "        with open(file_path, \"w\") as f:\n",
    "            # unique_acesses\n",
    "            s = str(unique_accesses)\n",
    "            f.write(s[1 : len(s) - 1] + \"\\n\")\n",
    "            # list_sd\n",
    "            s = str(list_sd)\n",
    "            f.write(s[1 : len(s) - 1] + \"\\n\")\n",
    "            # cumm_sd\n",
    "            s = str(cumm_sd)\n",
    "            f.write(s[1 : len(s) - 1] + \"\\n\")\n",
    "    except Exception:\n",
    "        print(\"Wrong file or file path\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    import os\n",
    "    import operator\n",
    "    import argparse\n",
    "\n",
    "    ### parse arguments ###\n",
    "    parser = argparse.ArgumentParser(description=\"Generate Synthetic Distributions\")\n",
    "    parser.add_argument(\"--trace-file\", type=str, default=\"./input/trace.log\")\n",
    "    parser.add_argument(\"--trace-file-binary-type\", type=bool, default=False)\n",
    "    parser.add_argument(\"--trace-enable-padding\", type=bool, default=False)\n",
    "    parser.add_argument(\"--dist-file\", type=str, default=\"./input/dist.log\")\n",
    "    parser.add_argument(\n",
    "        \"--synthetic-file\", type=str, default=\"./input/trace_synthetic.log\"\n",
    "    )\n",
    "    parser.add_argument(\"--numpy-rand-seed\", type=int, default=123)\n",
    "    parser.add_argument(\"--print-precision\", type=int, default=5)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    ### some basic setup ###\n",
    "    np.random.seed(args.numpy_rand_seed)\n",
    "    np.set_printoptions(precision=args.print_precision)\n",
    "\n",
    "    ### read trace ###\n",
    "    trace = read_trace_from_file(args.trace_file)\n",
    "    # print(trace)\n",
    "\n",
    "    ### profile trace ###\n",
    "    (_, stack_distances, line_accesses) = trace_profile(\n",
    "        trace, args.trace_enable_padding\n",
    "    )\n",
    "    stack_distances.reverse()\n",
    "    line_accesses.reverse()\n",
    "    # print(line_accesses)\n",
    "    # print(stack_distances)\n",
    "\n",
    "    ### compute probability distribution ###\n",
    "    # count items\n",
    "    l = len(stack_distances)\n",
    "    dc = sorted(\n",
    "        collections.Counter(stack_distances).items(), key=operator.itemgetter(0)\n",
    "    )\n",
    "\n",
    "    # create a distribution\n",
    "    list_sd = list(map(lambda tuple_x_k: tuple_x_k[0], dc))  # x = tuple_x_k[0]\n",
    "    dist_sd = list(\n",
    "        map(lambda tuple_x_k: tuple_x_k[1] / float(l), dc)\n",
    "    )  # k = tuple_x_k[1]\n",
    "    cumm_sd = []  # np.cumsum(dc).tolist() #prefixsum\n",
    "    for i, (_, k) in enumerate(dc):\n",
    "        if i == 0:\n",
    "            cumm_sd.append(k / float(l))\n",
    "        else:\n",
    "            # add the 2nd element of the i-th tuple in the dist_sd list\n",
    "            cumm_sd.append(cumm_sd[i - 1] + (k / float(l)))\n",
    "\n",
    "    ### write stack_distance and line_accesses to a file ###\n",
    "    write_dist_to_file(args.dist_file, line_accesses, list_sd, cumm_sd)\n",
    "\n",
    "    ### generate correspondinf synthetic ###\n",
    "    # line_accesses, list_sd, cumm_sd = read_dist_from_file(args.dist_file)\n",
    "    synthetic_trace = trace_generate_lru(\n",
    "        line_accesses, list_sd, cumm_sd, len(trace), args.trace_enable_padding\n",
    "    )\n",
    "    # synthetic_trace = trace_generate_rand(\n",
    "    #     line_accesses, list_sd, cumm_sd, len(trace), args.trace_enable_padding\n",
    "    # )\n",
    "    write_trace_to_file(args.synthetic_file, synthetic_trace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args for random and synthetic:\n",
    "use_gpu = False\n",
    "arch_sparse_feature_size = 16 # 2\n",
    "arch_embedding_size = \"4-3-2\"\n",
    "arch_mlp_bot = \"13-512-256-64-16\" #\"4-3-2\"\n",
    "arch_mlp_top = \"512-256-1\" #\"4-2-1\"\n",
    "arch_interaction_op = \"dot\"\n",
    "arch_interaction_itself = False\n",
    "\n",
    "activation_function = \"relu\"\n",
    "loss_function = \"mse\"\n",
    "loss_threshold = 0.0\n",
    "round_targets = False\n",
    "\n",
    "data_size = 1\n",
    "num_batches = 0\n",
    "data_generation = \"random\"\n",
    "data_trace_file = \"./input/dist_emb_j.log\"\n",
    "data_set = \"kaggle\"\n",
    "raw_data_file = \"\"\n",
    "processed_data_file = \"\"\n",
    "data_randomize = \"total\"\n",
    "data_trace_enable_padding = False\n",
    "num_indices_per_lookup = 10\n",
    "num_indices_per_lookup_fixed = True\n",
    "\n",
    "\n",
    "mini_batch_size = 2\n",
    "nepochs = 1\n",
    "learning_rate = 0.01\n",
    "print_precision = 5\n",
    "numpy_rand_seed = 123\n",
    "sync_dense_params = True\n",
    "\n",
    "inference_only = False\n",
    "save_onnx = False\n",
    "print_freq = 1\n",
    "test_freq = 1\n",
    "print_time = False\n",
    "debug_mode = False\n",
    "enable_profiling = False\n",
    "plot_compute_graph = False\n",
    "save_model = \"\"\n",
    "load_model = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment this and comment above cell for real criteo dataset:\n",
    "# use_gpu = False\n",
    "# arch_sparse_feature_size = 16\n",
    "# arch_embedding_size = \"4-3-2\"\n",
    "# arch_mlp_bot = \"13-512-256-64-16\"\n",
    "# arch_mlp_top = \"512-256-1\"\n",
    "# arch_interaction_op = \"dot\"\n",
    "# arch_interaction_itself = False\n",
    "\n",
    "# activation_function = \"relu\"\n",
    "# loss_function = \"bce\"\n",
    "# loss_threshold = 0.0\n",
    "# round_targets = True\n",
    "\n",
    "# data_size = 1\n",
    "# num_batches = 0\n",
    "# data_generation = \"dataset\"\n",
    "# data_trace_file = \"./input/dist_emb_j.log\"\n",
    "# data_set = \"kaggle\"\n",
    "# raw_data_file = \"./unprocesed/train.txt\" ## fill in the path with train.txt file\n",
    "# processed_data_file = \"\" ## once a master processed file is generated fill that file path here\n",
    "# data_randomize = \"total\"\n",
    "# data_trace_enable_padding = False\n",
    "# num_indices_per_lookup = 10\n",
    "# num_indices_per_lookup_fixed = True\n",
    "\n",
    "\n",
    "# mini_batch_size = 1\n",
    "# nepochs = 1\n",
    "# learning_rate = 0.2\n",
    "# print_precision = 5\n",
    "# numpy_rand_seed = 123\n",
    "# sync_dense_params = True\n",
    "\n",
    "# inference_only = False\n",
    "# save_onnx = True\n",
    "# print_freq = 1024\n",
    "# test_freq = 1\n",
    "# print_time = False\n",
    "# debug_mode = False\n",
    "# enable_profiling = True\n",
    "# plot_compute_graph = True\n",
    "# save_model = \"\"\n",
    "# load_model = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU...\n"
     ]
    }
   ],
   "source": [
    "# use_gpu = False\n",
    "### some basic setup ###\n",
    "np.random.seed(numpy_rand_seed)\n",
    "np.set_printoptions(precision=print_precision)\n",
    "torch.set_printoptions(precision=print_precision)\n",
    "torch.manual_seed(numpy_rand_seed)\n",
    "\n",
    "use_gpu = use_gpu and torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    torch.cuda.manual_seed_all(numpy_rand_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    device = torch.device(\"cuda\", 0)\n",
    "    ngpus = torch.cuda.device_count()  # 1\n",
    "    print(\"Using {} GPU(s)...\".format(ngpus))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prepare training data ###\n",
    "ln_bot = np.fromstring(arch_mlp_bot, dtype=int, sep=\"-\")\n",
    "# input data\n",
    "if data_generation == \"dataset\":\n",
    "    # input and target data\n",
    "    (\n",
    "        nbatches,\n",
    "        lX,\n",
    "        lS_o,\n",
    "        lS_i,\n",
    "        lT,\n",
    "        nbatches_test,\n",
    "        lX_test,\n",
    "        lS_o_test,\n",
    "        lS_i_test,\n",
    "        lT_test,\n",
    "        ln_emb,\n",
    "        m_den,\n",
    "    ) = read_dataset(\n",
    "        data_set,\n",
    "        mini_batch_size,\n",
    "        data_randomize,\n",
    "        num_batches,\n",
    "        True,\n",
    "        raw_data_file,\n",
    "        processed_data_file,\n",
    "        inference_only,\n",
    "    )\n",
    "    ln_bot[0] = m_den\n",
    "else:\n",
    "    # input data\n",
    "    ln_emb = np.fromstring(arch_embedding_size, dtype=int, sep=\"-\")\n",
    "    m_den = ln_bot[0]\n",
    "    if data_generation == \"random\":\n",
    "        (nbatches, lX, lS_o, lS_i) = generate_random_input_data(\n",
    "            data_size,\n",
    "            num_batches,\n",
    "            mini_batch_size,\n",
    "            round_targets,\n",
    "            num_indices_per_lookup,\n",
    "            num_indices_per_lookup_fixed,\n",
    "            m_den,\n",
    "            ln_emb,\n",
    "        )\n",
    "    elif data_generation == \"synthetic\":\n",
    "        (nbatches, lX, lS_o, lS_i) = generate_synthetic_input_data(\n",
    "            data_size,\n",
    "            num_batches,\n",
    "            mini_batch_size,\n",
    "            round_targets,\n",
    "            num_indices_per_lookup,\n",
    "            num_indices_per_lookup_fixed,\n",
    "            m_den,\n",
    "            ln_emb,\n",
    "            data_trace_file,\n",
    "            data_trace_enable_padding,\n",
    "        )\n",
    "    else:\n",
    "        sys.exit(\n",
    "            \"ERROR: --data-generation=\" + data_generation + \" is not supported\"\n",
    "        )\n",
    "\n",
    "    # target data\n",
    "    (nbatches, lT) = generate_random_output_data(\n",
    "        data_size,\n",
    "        num_batches,\n",
    "        mini_batch_size,\n",
    "        round_targets=round_targets,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### parse command line arguments ###\n",
    "m_spa = arch_sparse_feature_size\n",
    "num_fea = ln_emb.size + 1  # num sparse + num dense features\n",
    "m_den_out = ln_bot[ln_bot.size - 1]\n",
    "if arch_interaction_op == \"dot\":\n",
    "    # approach 1: all\n",
    "    # num_int = num_fea * num_fea + m_den_out\n",
    "    # approach 2: unique\n",
    "    if arch_interaction_itself:\n",
    "        num_int = (num_fea * (num_fea + 1)) // 2 + m_den_out\n",
    "    else:\n",
    "        num_int = (num_fea * (num_fea - 1)) // 2 + m_den_out\n",
    "elif arch_interaction_op == \"cat\":\n",
    "    num_int = num_fea * m_den_out\n",
    "else:\n",
    "    sys.exit(\n",
    "        \"ERROR: --arch-interaction-op=\"\n",
    "        + args.arch_interaction_op\n",
    "        + \" is not supported\"\n",
    "    )\n",
    "arch_mlp_top_adjusted = str(num_int) + \"-\" + arch_mlp_top\n",
    "ln_top = np.fromstring(arch_mlp_top_adjusted, dtype=int, sep=\"-\")\n",
    "# sanity check: feature sizes and mlp dimensions must match\n",
    "if m_den != ln_bot[0]:\n",
    "    sys.exit(\n",
    "        \"ERROR: arch-dense-feature-size \"\n",
    "        + str(m_den)\n",
    "        + \" does not match first dim of bottom mlp \"\n",
    "        + str(ln_bot[0])\n",
    "    )\n",
    "if m_spa != m_den_out:\n",
    "    sys.exit(\n",
    "        \"ERROR: arch-sparse-feature-size \"\n",
    "        + str(m_spa)\n",
    "        + \" does not match last dim of bottom mlp \"\n",
    "        + str(m_den_out)\n",
    "    )\n",
    "if num_int != ln_top[0]:\n",
    "    sys.exit(\n",
    "        \"ERROR: # of feature interactions \"\n",
    "        + str(num_int)\n",
    "        + \" does not match first dimension of top mlp \"\n",
    "        + str(ln_top[0])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test prints (model arch)\n",
    "if debug_mode:\n",
    "    print(\"model arch:\")\n",
    "    print(\n",
    "        \"mlp top arch \"\n",
    "        + str(ln_top.size - 1)\n",
    "        + \" layers, with input to output dimensions:\"\n",
    "    )\n",
    "    print(ln_top)\n",
    "    print(\"# of interactions\")\n",
    "    print(num_int)\n",
    "    print(\n",
    "        \"mlp bot arch \"\n",
    "        + str(ln_bot.size - 1)\n",
    "        + \" layers, with input to output dimensions:\"\n",
    "    )\n",
    "    print(ln_bot)\n",
    "    print(\"# of features (sparse and dense)\")\n",
    "    print(num_fea)\n",
    "    print(\"dense feature size\")\n",
    "    print(m_den)\n",
    "    print(\"sparse feature size\")\n",
    "    print(m_spa)\n",
    "    print(\n",
    "        \"# of embeddings (= # of sparse features) \"\n",
    "        + str(ln_emb.size)\n",
    "        + \", with dimensions \"\n",
    "        + str(m_spa)\n",
    "        + \"x:\"\n",
    "    )\n",
    "    print(ln_emb)\n",
    "\n",
    "    print(\"data (inputs and targets):\")\n",
    "    for j in range(0, nbatches):\n",
    "        print(\"mini-batch: %d\" % j)\n",
    "        print(lX[j].detach().cpu().numpy())\n",
    "        # transform offsets to lengths when printing\n",
    "        print(\n",
    "            [\n",
    "                np.diff(\n",
    "                    S_o.detach().cpu().tolist() + list(lS_i[j][i].shape)\n",
    "                ).tolist()\n",
    "                for i, S_o in enumerate(lS_o[j])\n",
    "            ]\n",
    "        )\n",
    "        print([S_i.detach().cpu().tolist() for S_i in lS_i[j]])\n",
    "        print(lT[j].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### construct the neural network specified above ###\n",
    "dlrm = DLRM_Net(\n",
    "    m_spa,\n",
    "    ln_emb,\n",
    "    ln_bot,\n",
    "    ln_top,\n",
    "    arch_interaction_op=arch_interaction_op,\n",
    "    arch_interaction_itself=arch_interaction_itself,\n",
    "    sigmoid_bot=-1,\n",
    "    sigmoid_top=ln_top.size - 2,\n",
    "    sync_dense_params=sync_dense_params,\n",
    "    loss_threshold=loss_threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test prints\n",
    "if debug_mode:\n",
    "    print(\"initial parameters (weights and bias):\")\n",
    "    for param in dlrm.parameters():\n",
    "        print(param.detach().cpu().numpy())\n",
    "    # print(dlrm)\n",
    "\n",
    "if use_gpu:\n",
    "    if ngpus > 1:\n",
    "        # Custom Model-Data Parallel\n",
    "        # the mlps are replicated and use data parallelism, while\n",
    "        # the embeddings are distributed and use model parallelism\n",
    "        dlrm.ndevices = min(ngpus, mini_batch_size, num_fea - 1)\n",
    "    dlrm = dlrm.to(device)  # .cuda()\n",
    "\n",
    "# specify the loss function\n",
    "if loss_function == \"mse\":\n",
    "    loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
    "elif loss_function == \"bce\":\n",
    "    loss_fn = torch.nn.BCELoss(reduction=\"mean\")\n",
    "else:\n",
    "    sys.exit(\"ERROR: --loss-function=\" + loss_function + \" is not supported\")\n",
    "\n",
    "if not inference_only:\n",
    "    # specify the optimizer algorithm\n",
    "    optimizer = torch.optim.SGD(dlrm.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### main loop ###\n",
    "def time_wrap(use_gpu):\n",
    "    if use_gpu:\n",
    "        torch.cuda.synchronize()\n",
    "    return time.time()\n",
    "\n",
    "def dlrm_wrap(X, lS_o, lS_i, use_gpu, device):\n",
    "    if use_gpu:  # .cuda()\n",
    "        return dlrm(\n",
    "            X.to(device),\n",
    "            [S_o.to(device) for S_o in lS_o],\n",
    "            [S_i.to(device) for S_i in lS_i],\n",
    "        )\n",
    "    else:\n",
    "        return dlrm(X, lS_o, lS_i)\n",
    "\n",
    "def loss_fn_wrap(Z, T, use_gpu, device):\n",
    "    if use_gpu:\n",
    "        return loss_fn(Z, T.to(device))\n",
    "    else:\n",
    "        return loss_fn(Z, T)\n",
    "\n",
    "    # training\n",
    "\n",
    "best_gA_test = 0\n",
    "total_time = 0\n",
    "total_loss = 0\n",
    "total_accu = 0\n",
    "total_iter = 0\n",
    "k = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model is specified\n",
    "if not (load_model == \"\"):\n",
    "    print(\"Loading saved mode {}\".format(load_model))\n",
    "    ld_model = torch.load(load_model)\n",
    "    dlrm.load_state_dict(ld_model[\"state_dict\"])\n",
    "    ld_j = ld_model[\"iter\"]\n",
    "    ld_k = ld_model[\"epoch\"]\n",
    "    ld_nepochs = ld_model[\"nepochs\"]\n",
    "    ld_nbatches = ld_model[\"nbatches\"]\n",
    "    ld_nbatches_test = ld_model[\"nbatches_test\"]\n",
    "    ld_gA = ld_model[\"train_acc\"]\n",
    "    ld_gL = ld_model[\"train_loss\"]\n",
    "    ld_total_loss = ld_model[\"total_loss\"]\n",
    "    ld_total_accu = ld_model[\"total_accu\"]\n",
    "    ld_gA_test = ld_model[\"test_acc\"]\n",
    "    ld_gL_test = ld_model[\"test_loss\"]\n",
    "    if not inference_only:\n",
    "        optimizer.load_state_dict(ld_model[\"opt_state_dict\"])\n",
    "        best_gA_test = ld_gA_test\n",
    "        total_loss = ld_total_loss\n",
    "        total_accu = ld_total_accu\n",
    "        k = ld_k  # epochs\n",
    "        j = ld_j  # batches\n",
    "    else:\n",
    "        print_freq = ld_nbatches\n",
    "        test_freq = 0\n",
    "    print(\n",
    "        \"Saved model Training state: epoch = {:d}/{:d}, batch = {:d}/{:d}, train loss = {:.6f}, train accuracy = {:3.3f} %\".format(\n",
    "            ld_k, ld_nepochs, ld_j, ld_nbatches, ld_gL, ld_gA * 100\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Saved model Testing state: nbatches = {:d}, test loss = {:.6f}, test accuracy = {:3.3f} %\".format(\n",
    "            ld_nbatches_test, ld_gL_test, ld_gA_test * 100\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time/loss/accuracy (if enabled):\n",
      "Finished training it 1/1 of epoch 0, -1.00 ms/it, loss 0.071748, accuracy 0.000 %\n"
     ]
    }
   ],
   "source": [
    "print(\"time/loss/accuracy (if enabled):\")\n",
    "with torch.autograd.profiler.profile(enable_profiling, use_gpu) as prof:\n",
    "    while k < nepochs:\n",
    "        j = 0\n",
    "        while j < nbatches:\n",
    "            t1 = time_wrap(use_gpu)\n",
    "\n",
    "            # forward pass\n",
    "            Z = dlrm_wrap(lX[j], lS_o[j], lS_i[j], use_gpu, device)\n",
    "\n",
    "            # loss\n",
    "            E = loss_fn_wrap(Z, lT[j], use_gpu, device)\n",
    "\n",
    "            # compute loss and accuracy\n",
    "            L = E.detach().cpu().numpy()  # numpy array\n",
    "            S = Z.detach().cpu().numpy()  # numpy array\n",
    "            T = lT[j].detach().cpu().numpy()  # numpy array\n",
    "            mbs = T.shape[0]  # = args.mini_batch_size except maybe for last\n",
    "            A = np.sum((np.round(S, 0) == T).astype(np.uint8)) / mbs\n",
    "\n",
    "            if not inference_only:\n",
    "                # scaled error gradient propagation\n",
    "                # (where we do not accumulate gradients across mini-batches)\n",
    "                optimizer.zero_grad()\n",
    "                # backward pass\n",
    "                E.backward()\n",
    "                # debug prints (check gradient norm)\n",
    "                # for l in mlp.layers:\n",
    "                #     if hasattr(l, 'weight'):\n",
    "                #          print(l.weight.grad.norm().item())\n",
    "\n",
    "                # optimizer\n",
    "                optimizer.step()\n",
    "\n",
    "            t2 = time_wrap(use_gpu)\n",
    "            total_time += t2 - t1\n",
    "            total_accu += A\n",
    "            total_loss += L\n",
    "            total_iter += 1\n",
    "\n",
    "            print_tl = ((j + 1) % print_freq == 0) or (j + 1 == nbatches)\n",
    "            print_ts = (\n",
    "                (test_freq > 0)\n",
    "                and (data_generation == \"dataset\")\n",
    "                and (((j + 1) % test_freq == 0) or (j + 1 == nbatches))\n",
    "            )\n",
    "\n",
    "            # print time, loss and accuracy\n",
    "            if print_tl or print_ts:\n",
    "                gT = 1000.0 * total_time / total_iter if print_time else -1\n",
    "                total_time = 0\n",
    "\n",
    "                gL = total_loss / total_iter\n",
    "                total_loss = 0\n",
    "\n",
    "                gA = total_accu / total_iter\n",
    "                total_accu = 0\n",
    "\n",
    "                str_run_type = \"inference\" if inference_only else \"training\"\n",
    "                print(\n",
    "                    \"Finished {} it {}/{} of epoch {}, \".format(\n",
    "                        str_run_type, j + 1, nbatches, k\n",
    "                    )\n",
    "                    + \"{:.2f} ms/it, loss {:.6f}, accuracy {:3.3f} %\".format(\n",
    "                        gT, gL, gA * 100\n",
    "                    )\n",
    "                )\n",
    "                total_iter = 0\n",
    "\n",
    "            # testing\n",
    "            if print_ts and not inference_only:\n",
    "                test_accu = 0\n",
    "                test_loss = 0\n",
    "\n",
    "                for jt in range(0, nbatches_test):\n",
    "                    t1_test = time_wrap(use_gpu)\n",
    "\n",
    "                    # forward pass\n",
    "                    Z_test = dlrm_wrap(\n",
    "                        lX_test[jt], lS_o_test[jt], lS_i_test[jt], use_gpu, device\n",
    "                    )\n",
    "                    # loss\n",
    "                    E_test = loss_fn_wrap(Z_test, lT_test[jt], use_gpu, device)\n",
    "\n",
    "                    # compute loss and accuracy\n",
    "                    L_test = E_test.detach().cpu().numpy()  # numpy array\n",
    "                    S_test = Z_test.detach().cpu().numpy()  # numpy array\n",
    "                    T_test = lT_test[jt].detach().cpu().numpy()  # numpy array\n",
    "                    mbs_test = T_test.shape[\n",
    "                        0\n",
    "                    ]  # = args.mini_batch_size except maybe for last\n",
    "                    A_test = (\n",
    "                        np.sum((np.round(S_test, 0) == T_test).astype(np.uint8))\n",
    "                        / mbs_test\n",
    "                    )\n",
    "\n",
    "                    t2_test = time_wrap(use_gpu)\n",
    "\n",
    "                    test_accu += A_test\n",
    "                    test_loss += L_test\n",
    "\n",
    "                gL_test = test_loss / nbatches_test\n",
    "                gA_test = test_accu / nbatches_test\n",
    "\n",
    "                is_best = gA_test > best_gA_test\n",
    "                if is_best:\n",
    "                    best_gA_test = gA_test\n",
    "                    if not (save_model == \"\"):\n",
    "                        print(\"Saving model to {}\".format(save_model))\n",
    "                        torch.save(\n",
    "                            {\n",
    "                                \"epoch\": k,\n",
    "                                \"nepochs\": nepochs,\n",
    "                                \"nbatches\": nbatches,\n",
    "                                \"nbatches_test\": nbatches_test,\n",
    "                                \"iter\": j + 1,\n",
    "                                \"state_dict\": dlrm.state_dict(),\n",
    "                                \"train_acc\": gA,\n",
    "                                \"train_loss\": gL,\n",
    "                                \"test_acc\": gA_test,\n",
    "                                \"test_loss\": gL_test,\n",
    "                                \"total_loss\": total_loss,\n",
    "                                \"total_accu\": total_accu,\n",
    "                                \"opt_state_dict\": optimizer.state_dict(),\n",
    "                            },\n",
    "                            save_model,\n",
    "                        )\n",
    "\n",
    "                print(\n",
    "                    \"Testing at - {}/{} of epoch {}, \".format(j + 1, nbatches, 0)\n",
    "                    + \"loss {:.6f}, accuracy {:3.3f} %, best {:3.3f} %\".format(\n",
    "                        gL_test, gA_test * 100, best_gA_test * 100\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            j += 1  # nbatches\n",
    "        k += 1  # nepochs\n",
    "# profiling\n",
    "if enable_profiling:\n",
    "    with open(\"dlrm_s_pytorch.prof\", \"w\") as prof_f:\n",
    "        prof_f.write(prof.key_averages().table(sort_by=\"cpu_time_total\"))\n",
    "        prof.export_chrome_trace(\"./dlrm_s_pytorch.json\")\n",
    "    # print(prof.key_averages().table(sort_by=\"cpu_time_total\"))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### uncomment for profiling and plotting and saving in  onnx\n",
    "# # # profiling\n",
    "# # if enable_profiling:\n",
    "# #     with open(\"dlrm_s_pytorch.prof\", \"w\") as prof_f:\n",
    "# #         prof_f.write(prof.key_averages().table(sort_by=\"cpu_time_total\"))\n",
    "# #         prof.export_chrome_trace(\"./dlrm_s_pytorch.json\")\n",
    "# #     # print(prof.key_averages().table(sort_by=\"cpu_time_total\"))\n",
    "\n",
    "# # # plot compute graph\n",
    "# # if plot_compute_graph:\n",
    "# # #     sys.exit(\n",
    "# # #         \"ERROR: Please install pytorchviz package in order to use the\"\n",
    "# # #         + \" visualization. Then, uncomment its import above as well as\"\n",
    "# # #         + \" three lines below and run the code again.\"\n",
    "# # #     )\n",
    "# #     V = Z.mean() if inference_only else Parameter(torch.Tensor(L))\n",
    "# #     dot = make_dot(V, params=dict(dlrm.named_parameters()))\n",
    "# #     dot.render('dlrm_s_pytorch_graph') # write .pdf file\n",
    "\n",
    "# # test prints\n",
    "# if not inference_only and debug_mode:\n",
    "#     print(\"updated parameters (weights and bias):\")\n",
    "#     for param in dlrm.parameters():\n",
    "#         print(param.detach().cpu().numpy())\n",
    "\n",
    "# if save_onnx:\n",
    "#     # export the model in onnx\n",
    "#     with open(\"dlrm_s_pytorch.onnx\", \"w+b\") as dlrm_pytorch_onnx_file:\n",
    "#         torch.onnx._export(\n",
    "#             dlrm, (lX[0], lS_o[0], lS_i[0]), dlrm_pytorch_onnx_file, verbose=True\n",
    "#         )\n",
    "#     # recover the model back\n",
    "#     dlrm_pytorch_onnx = onnx.load(\"dlrm_s_pytorch.onnx\")\n",
    "#     # check the onnx model\n",
    "#     onnx.checker.check_model(dlrm_pytorch_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dlrm,\"dlrm_synthetic_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlrm_infer = torch.load(\"dlrm_synthetic_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DLRM_Net(\n",
       "  (emb_l): ModuleList(\n",
       "    (0): EmbeddingBag(4, 16, mode=sum)\n",
       "    (1): EmbeddingBag(3, 16, mode=sum)\n",
       "    (2): EmbeddingBag(2, 16, mode=sum)\n",
       "  )\n",
       "  (bot_l): Sequential(\n",
       "    (0): Linear(in_features=13, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (7): ReLU()\n",
       "  )\n",
       "  (top_l): Sequential(\n",
       "    (0): Linear(in_features=22, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlrm_infer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lS_i[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([0, 3]), tensor([0, 3]), tensor([0, 2])]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lS_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([0, 1, 2, 0, 1, 2, 3]),\n",
       "  tensor([0, 1, 2, 0, 1, 2]),\n",
       "  tensor([0, 1, 0, 1])]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lS_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.14780]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### predicting click probabilities \n",
    "\n",
    "dlrm_infer(lX[0], lS_o[0], lS_i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
